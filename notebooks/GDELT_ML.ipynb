{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate class Weight\n",
    "# TODO: Make Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.keras._impl.keras.layers import Dense, Input, LSTM, BatchNormalization, Flatten, TimeDistributed, Dropout, Reshape\n",
    "from tensorflow.python.keras._impl.keras.models import Model\n",
    "from tensorflow.python.keras._impl.keras.utils import Sequence\n",
    "from tensorflow.python.keras._impl.keras.callbacks import CSVLogger, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDELTSequencer(Sequence):\n",
    "\n",
    "    def __init__(self, datas, batch_size):\n",
    "        \"\"\"\n",
    "        Instantiate the class\n",
    "        :param datas: The list of s3 path for all the csv files \n",
    "        :param batch_size: The Number of images to return by batch\n",
    "        \"\"\"\n",
    "        self.datas = datas\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get The len of the current batch\n",
    "        :return: The len of th current batch\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.datas) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "        \"\"\"\n",
    "        Get a batch item in the Sequencer\n",
    "        :param idx: The index requested\n",
    "        :return: A tuple of batch\n",
    "        \"\"\"\n",
    "        dl = list()\n",
    "        batch_datas = self.datas[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        for el in batch_datas:\n",
    "            dcsv = pd.read_csv('s3://gdelt4ibd/' + el, engine='c', low_memory=True,\n",
    "                               dtype=np.float32, compression='gzip')\n",
    "            temp_train = dcsv[['date'] + [c for c in list(dcsv.columns) if 'actor1code' or 'actor2code' in c]]\n",
    "            temp_test = dcsv[[c for c in list(dcsv.columns) if 'event' or 'geo' in c]]\n",
    "            dl.append((temp_train, temp_test))\n",
    "        \n",
    "        train = pd.concat([t[0] for t in dl], ignore_index=True)\n",
    "        test = pd.concat([t[1] for t in dl], ignore_index=True)\n",
    "        \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Regression model for The GDELT\n",
    "    :param input_shape: The input shape of the data\n",
    "    :param num_classes: The number of classes\n",
    "    :return: A compiled model\n",
    "    \"\"\"\n",
    "    print('Designing model')\n",
    "    inputs = Input(input_shape)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(inputs)\n",
    "    model = Model(inputs, outputs)\n",
    "    # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Connecting to S3')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "content_object = s3.Object('gdelt4ibd', 'gdelt_metadata.json')\n",
    "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "gdelt_metadata = json.loads(file_content)\n",
    "\n",
    "gdelt_bucket = s3.Bucket('gdelt4ibd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reading GDELT\n",
      "Normalizing GDELT\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6d75c278286d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgdelt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_gdelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexperiment_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'REGRESSION_MODEL_ADAM_BIN_SIG'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnb_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6dd4f3fd2e88>\u001b[0m in \u001b[0;36mread_gdelt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgdelt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SQLDATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdelt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SQLDATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'20170101'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'20190101'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     gdelt_train = np.asarray(pd.get_dummies(gdelt[['SQLDATE', 'Actor1Code', 'Actor2Code']],\n\u001b[0;32m---> 16\u001b[0;31m                                             columns=['Actor1Code', 'Actor2Code']).values, dtype=np.float32)\n\u001b[0m\u001b[1;32m     17\u001b[0m     gdelt_val = np.asarray(pd.get_dummies(gdelt[['EventCode', 'Actor1Geo_CountryCode', 'Actor2Geo_CountryCode',\n\u001b[1;32m     18\u001b[0m                                           'ActionGeo_CountryCode']]).values, dtype=np.float32)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    878\u001b[0m             dummy = _get_dummies_1d(col[1], prefix=pre, prefix_sep=sep,\n\u001b[1;32m    879\u001b[0m                                     \u001b[0mdummy_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy_na\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                                     drop_first=drop_first, dtype=dtype)\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0mwith_dummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0mdummy_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Defining Variables')\n",
    "experiment_name = 'REGRESSION_MODEL_ADAM_BIN_SIG'\n",
    "batch_size = 8096\n",
    "nb_epochs = 64\n",
    "num_classes = gdelt_metadata['num_classes']\n",
    "shape = gdelt_metadata['train_shape']\n",
    "csv_logger = CSVLogger('s3://gdelt4ibd/models/' + experiment_name + '.csv')\n",
    "checkpoint = ModelCheckpoint('s3://gdelt4ibd/models/' + experiment_name + '.h5',\n",
    "                             monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "gdelt_model = regression_model((shape[1:],), num_classes)\n",
    "gen_gdelt = GDELTSequencer([object_summary.key for object_summary\n",
    "                            in gdelt_bucket.objects.filter(Prefix='normalized_gdelt_2017_2018.csv'):],\n",
    "                           batch_size)\n",
    "\n",
    "print('Starting learning')\n",
    "gdelt_model.fit_generator(generator=gen_gdelt, epochs=nb_epochs, shuffle=True, callbacks=[csv_logger, checkpoint]\n",
    "                          steps_per_epoch=shape[0] / batch_size, use_multiprocessing=True, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
